{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d4db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_FRACTION = 1  # 改成 0.1 即表示使用清洗后训练集的 10%\n",
    "\n",
    "'''\n",
    "# 如果还没装依赖\n",
    "!pip -q install pyarrow\n",
    "'''\n",
    "\n",
    "# =========================================================\n",
    "# 0) 读取数据\n",
    "# =========================================================\n",
    "#parquet_path = \"/content/drive/MyDrive/CA-AA/Cleandata_Lable20_N157794_ML30_MU3000.parquet\" # 20251201\n",
    "#parquet_path = \"/content/drive/MyDrive/CA-AA/Cleandata5_Lable20_N54820_3000_ML30_MU3000.parquet\"  # 20251207-1\n",
    "#parquet_path = \"/content/drive/MyDrive/CA-AA/Cleandata5_Lable20_N70848_4000_ML30_MU3000.parquet\"  # 20251207-2\n",
    "parquet_path = \"/content/drive/MyDrive/CA-AA/Cleandata5_Lable20_N83597_5000_ML30_MU3000.parquet\"   # 20251207-3\n",
    "\n",
    "data = pd.read_parquet(parquet_path)\n",
    "print(\"读取完成:\", data.shape)\n",
    "\n",
    "# =========================================================\n",
    "# 1) 有效长度+按标签剔除\n",
    "# =========================================================\n",
    "label_col = data.columns[0]      # 第一列 label\n",
    "feature_cols = data.columns[3:]  # 第四列开始是时序特征\n",
    "\n",
    "data = data.copy()\n",
    "data[\"valid_length\"] = (data[feature_cols] != 0).sum(axis=1)\n",
    "\n",
    "def remove_outliers_by_label(df):\n",
    "    q_low = df[\"valid_length\"].quantile(0.35)\n",
    "    q_high = df[\"valid_length\"].quantile(0.65)\n",
    "    return df[(df[\"valid_length\"] >= q_low) & (df[\"valid_length\"] <= q_high)]\n",
    "\n",
    "cleaned_data = (\n",
    "    data\n",
    "    .groupby(label_col, group_keys=False)\n",
    "    .apply(remove_outliers_by_label)\n",
    ")\n",
    "\n",
    "cleaned_data = cleaned_data.drop(columns=[\"valid_length\"])\n",
    "print(f\"清洗前样本数: {len(data)}\")\n",
    "print(f\"清洗后样本数: {len(cleaned_data)}\")\n",
    "\n",
    "data = cleaned_data\n",
    "\n",
    "# =========================================================\n",
    "# 2) 删除指定标签\n",
    "# =========================================================\n",
    "remove_labels = [8, 12, 14]\n",
    "filtered_data = data[~data.iloc[:, 0].isin(remove_labels)].copy()\n",
    "\n",
    "print(\"原始数据 shape:\", data.shape)\n",
    "print(\"删除后的数据 shape:\", filtered_data.shape)\n",
    "data = filtered_data\n",
    "\n",
    "# =========================================================\n",
    "# 3) 划分 train/val/test\n",
    "# =========================================================\n",
    "labels = data.iloc[:, 0].values\n",
    "raw = data.iloc[:, 3:].values         # numpy array [N, L]\n",
    "\n",
    "del data\n",
    "gc.collect()\n",
    "\n",
    "# 先拆训练/临时\n",
    "raw_train, raw_tmp, y_train, y_tmp = train_test_split(\n",
    "    raw, labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "# 再拆临时为 验证/测试\n",
    "raw_val, raw_test, y_val, y_test = train_test_split(\n",
    "    raw_tmp, y_tmp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_tmp\n",
    ")\n",
    "\n",
    "del raw, raw_tmp, y_tmp\n",
    "gc.collect()\n",
    "\n",
    "print(f\"训练集样本数（拆分后）：{len(raw_train)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 4) 第一次清洗：按非零长度去除每标签最小/最大 x% 样本\n",
    "# =========================================================\n",
    "x = 0.0  # 原来是 0 或 0.005，自行调整\n",
    "keep_idx = []\n",
    "for lbl in np.unique(y_train):\n",
    "    idxs = np.where(y_train == lbl)[0]\n",
    "    nz = (raw_train[idxs] != 0).sum(axis=1)\n",
    "    order = idxs[np.argsort(nz)]\n",
    "    n = len(order)\n",
    "    low, high = int(n * x), int(n * (1 - x))\n",
    "    keep_idx.extend(order[low:high])\n",
    "\n",
    "keep_idx = np.sort(keep_idx)\n",
    "raw_train = raw_train[keep_idx]\n",
    "y_train   = y_train[keep_idx]\n",
    "gc.collect()\n",
    "\n",
    "# =========================================================\n",
    "# 5) 第二次清洗：按“最大下降幅度”去除每标签最小/最大 x% 样本\n",
    "# =========================================================\n",
    "x = 0.0  # 同上\n",
    "\n",
    "def calc_max_drop_amp(arr):\n",
    "    v = arr[~np.isnan(arr) & (arr != 0)]\n",
    "    if v.size == 0:\n",
    "        return 0.0\n",
    "    m = np.argmax(v)\n",
    "    s = max(0, m - 10)\n",
    "    e = min(v.size, m + 11)\n",
    "    return abs(v[s:e].mean())\n",
    "\n",
    "amps = np.apply_along_axis(calc_max_drop_amp, 1, raw_train)\n",
    "keep2 = []\n",
    "for lbl in np.unique(y_train):\n",
    "    idxs = np.where(y_train == lbl)[0]\n",
    "    order = idxs[np.argsort(amps[idxs])]\n",
    "    n = len(order)\n",
    "    low, high = int(n * x), int(n * (1 - x))\n",
    "    keep2.extend(order[low:high])\n",
    "\n",
    "keep2 = np.sort(keep2)\n",
    "raw_train = raw_train[keep2]\n",
    "y_train   = y_train[keep2]\n",
    "\n",
    "del amps, keep_idx, keep2\n",
    "gc.collect()\n",
    "\n",
    "print(f\"训练集样本数（清洗后，尚未下采样）：{len(raw_train)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 5.1) 训练集下采样：用 TRAIN_FRACTION 控制规模\n",
    "# =========================================================\n",
    "\n",
    "if 0.0 < TRAIN_FRACTION < 1.0:\n",
    "    rng = np.random.RandomState(42)\n",
    "    selected_idx = []\n",
    "\n",
    "    # 按标签分层下采样，尽量保持类别分布\n",
    "    for lbl in np.unique(y_train):\n",
    "        idxs = np.where(y_train == lbl)[0]\n",
    "        n_lbl = len(idxs)\n",
    "        n_keep_lbl = max(1, int(n_lbl * TRAIN_FRACTION))\n",
    "        chosen = rng.choice(idxs, size=n_keep_lbl, replace=False)\n",
    "        selected_idx.extend(chosen)\n",
    "\n",
    "    selected_idx = np.array(selected_idx)\n",
    "    raw_train = raw_train[selected_idx]\n",
    "    y_train   = y_train[selected_idx]\n",
    "\n",
    "print(f\"训练集样本数（下采样后，比例={TRAIN_FRACTION:.2f}）：{len(raw_train)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 6) 标准化：仅用训练集 fit，然后 transform 三个子集\n",
    "# =========================================================\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(raw_train)                 # 只用训练集\n",
    "X_train_np = scaler.transform(raw_train)\n",
    "X_val_np   = scaler.transform(raw_val)\n",
    "X_test_np  = scaler.transform(raw_test)\n",
    "\n",
    "del raw_train, raw_val, raw_test\n",
    "gc.collect()\n",
    "\n",
    "# 转为 torch.Tensor\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "X_val   = torch.tensor(X_val_np,   dtype=torch.float32)\n",
    "X_test  = torch.tensor(X_test_np,  dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train,    dtype=torch.long)\n",
    "y_val   = torch.tensor(y_val,      dtype=torch.long)\n",
    "y_test  = torch.tensor(y_test,     dtype=torch.long)\n",
    "\n",
    "del X_train_np, X_val_np, X_test_np\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7) 通道构造：多滑动窗口通道\n",
    "# =========================================================\n",
    "use_sliding_std = 1     # 打开滑动窗口  # 1：使用滑动标准差双通道；0：只用单通道\n",
    "windows = [10,20,40]      # 想要使用的滑动窗口：10 和 20\n",
    "\n",
    "def sliding_std(x: torch.Tensor, window_size: int) -> torch.Tensor:\n",
    "    patches = x.unfold(1, window_size, 1)          # (N, L-ws+1, ws)\n",
    "    stds    = patches.std(dim=2, unbiased=False)   # (N, L-ws+1)\n",
    "    pad_l   = (window_size - 1) // 2\n",
    "    pad_r   = window_size - 1 - pad_l\n",
    "    return F.pad(stds, (pad_l, pad_r), value=0.0)  # (N, L)\n",
    "\n",
    "if use_sliding_std:\n",
    "    channels_train = [X_train]  # 通道 0：原始信号\n",
    "    channels_val   = [X_val]\n",
    "    channels_test  = [X_test]\n",
    "\n",
    "    for ws in windows:\n",
    "        std_tr = sliding_std(X_train, ws)\n",
    "        std_v  = sliding_std(X_val,   ws)\n",
    "        std_te = sliding_std(X_test,  ws)\n",
    "        channels_train.append(std_tr)\n",
    "        channels_val.append(std_v)\n",
    "        channels_test.append(std_te)\n",
    "\n",
    "    X_train = torch.stack(channels_train, dim=1)  # [N, C, L]，C = 1 + len(windows)\n",
    "    X_val   = torch.stack(channels_val, dim=1)\n",
    "    X_test  = torch.stack(channels_test, dim=1)\n",
    "\n",
    "    del std_tr, std_v, std_te\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    X_train = X_train.unsqueeze(1)\n",
    "    X_val   = X_val.unsqueeze(1)\n",
    "    X_test  = X_test.unsqueeze(1)\n",
    "\n",
    "print(f\"训练集样本数：{X_train.size(0)}, 通道数：{X_train.size(1)}, 长度：{X_train.size(2)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################################################################训练\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====== Attention After TCN (unchanged) ======\n",
    "class AttentionAfterTCN(nn.Module):\n",
    "    def __init__(self, channel_in, window_size=100, stride=100,\n",
    "                 hidden_dim=32, num_heads=2, dropout=0):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.linear_proj = nn.Linear(channel_in * window_size, hidden_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=hidden_dim,\n",
    "                                          num_heads=num_heads,\n",
    "                                          dropout=dropout,\n",
    "                                          batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, L = x.shape\n",
    "        N = (L - self.window_size) // self.stride + 1\n",
    "\n",
    "        # 1) unfold into windows\n",
    "        x_unfold = x.unfold(2, self.window_size, self.stride)           # (B, C, N, W)\n",
    "        x_unfold = x_unfold.permute(0,2,1,3).reshape(B, N, C*self.window_size)\n",
    "\n",
    "        # 2) linear + ReLU\n",
    "        x_proj = F.relu(self.linear_proj(x_unfold))                    # (B, N, hidden_dim)\n",
    "\n",
    "        # 3) self-attention\n",
    "        attn_out, _ = self.attn(x_proj, x_proj, x_proj)                # (B, N, hidden_dim)\n",
    "\n",
    "        # 4) residual\n",
    "        if attn_out.shape == x_proj.shape:\n",
    "            attn_out = attn_out + x_proj\n",
    "\n",
    "        # 5) mean-pool over windows + dropout\n",
    "        out = attn_out.mean(dim=1)                                     # (B, hidden_dim)\n",
    "        return self.dropout(out)\n",
    "\n",
    "# ====== TCNWithAttention using your TCN backbone ======\n",
    "class TCNWithAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_channels=1,\n",
    "                 output_channels=128,\n",
    "                 kernel_size=3,\n",
    "                 num_layers=8,\n",
    "                 num_classes=20,\n",
    "                 threshold=0.9,\n",
    "                 window_size=50,\n",
    "                 stride=50,\n",
    "                 attn_dim=512,\n",
    "                 num_heads=8,\n",
    "                 dropout=0.8):\n",
    "        super().__init__()\n",
    "        # TCN backbone: conv -> ReLU -> pool each layer\n",
    "        self.tcn_layers = nn.ModuleList()\n",
    "        in_ch = input_channels\n",
    "        for i in range(num_layers):\n",
    "            dilation = 2 ** i\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv1d(in_ch, output_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          padding=padding,\n",
    "                          dilation=dilation),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=2)\n",
    "            )\n",
    "            self.tcn_layers.append(block)\n",
    "            in_ch = output_channels\n",
    "\n",
    "        # Attention & classifier\n",
    "        self.attn_block = AttentionAfterTCN(output_channels,\n",
    "                                            window_size, stride,\n",
    "                                            attn_dim, num_heads, dropout)\n",
    "        self.fc = nn.Linear(attn_dim, num_classes)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        for block in self.tcn_layers:\n",
    "            x = block(x)   # conv -> relu -> pool\n",
    "\n",
    "        # align length to windows\n",
    "        L = x.size(2)\n",
    "        N = (L - self.attn_block.window_size) // self.attn_block.stride + 1\n",
    "        needed = N * self.attn_block.stride + self.attn_block.window_size\n",
    "        x = x[:, :, :needed]\n",
    "\n",
    "        # Attention + classifier\n",
    "        out = self.attn_block(x)        # (B, attn_dim)\n",
    "        logits = self.fc(out)           # (B, num_classes)\n",
    "        return logits                   # raw logits\n",
    "\n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = torch.max(probs, dim=1)\n",
    "        pred[conf < self.threshold] = -1\n",
    "        return pred, conf\n",
    "\n",
    "# ====== Dynamic Class Weight Tracker (unchanged) ======\n",
    "class DynamicClassWeightTracker:\n",
    "    def __init__(self, num_classes, device,\n",
    "                 alpha=0.9, epsilon=1e-6,\n",
    "                 weight_min=0.5, weight_max=1000.0):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_min = weight_min\n",
    "        self.weight_max = weight_max\n",
    "        self.smoothed_accuracies = torch.ones(\n",
    "            num_classes, dtype=torch.float32).to(device)\n",
    "\n",
    "    def update(self, preds, targets):\n",
    "        for c in range(self.num_classes):\n",
    "            mask = (targets == c)\n",
    "            total = mask.sum().item()\n",
    "            correct = (preds[mask] == targets[mask]).sum().item()\n",
    "            acc = correct / total if total > 0 else 1.0\n",
    "            self.smoothed_accuracies[c] = (\n",
    "                self.alpha * self.smoothed_accuracies[c]\n",
    "                + (1 - self.alpha) * acc\n",
    "            )\n",
    "        weights = 1.0 / (self.smoothed_accuracies + self.epsilon)\n",
    "        return torch.clamp(weights, self.weight_min, self.weight_max)\n",
    "\n",
    "# ====== Training Preparation ======\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")                         ########### ！！！！！！！！！！！！！！！！！！！！！\n",
    "model = TCNWithAttention(\n",
    "    input_channels=X_train.size(1),\n",
    "    output_channels=64,\n",
    "    kernel_size=3,\n",
    "    num_layers=5,\n",
    "    num_classes=20,\n",
    "    threshold=0.9,\n",
    "    window_size=5,\n",
    "    stride=5,\n",
    "    attn_dim=128,\n",
    "    num_heads=8,\n",
    "    dropout=0.7\n",
    ").to(device)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train),\n",
    "                          batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val,   y_val),\n",
    "                          batch_size=64)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-3,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=40,\n",
    "    pct_start=0.1,\n",
    "    anneal_strategy='cos',\n",
    "    final_div_factor=1e4\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tracker = DynamicClassWeightTracker(num_classes=20, device=device)\n",
    "\n",
    "train_accs, val_accs_max, val_accs_pred, coverages = [], [], [], []\n",
    "\n",
    "# ====== Training Loop ======\n",
    "for epoch in range(40):\n",
    "    # —— train ——\n",
    "    model.train()\n",
    "    correct = total = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss   = criterion(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    train_accs.append(100 * correct / total)\n",
    "\n",
    "   ###\n",
    "    # —— validate ——\n",
    "    model.eval()\n",
    "    cm_max = cm_pred = total_max = total_pred = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            # Val Acc(max)\n",
    "            _, pm = torch.max(logits, dim=1)\n",
    "            cm_max += (pm == yb).sum().item()\n",
    "            total_max += yb.size(0)\n",
    "            # Val Acc(pred)\n",
    "            pp, conf = model.predict(xb)\n",
    "            mask = (pp != -1)   # ✅ 用预测标签是否为 -1 来筛选“被接受”的样本\n",
    "            if mask.any():\n",
    "                cm_pred   += (pp[mask] == yb[mask]).sum().item()   # 只在“被接受”的样本上计算准确率\n",
    "                total_pred += mask.sum().item()                    # 分母 = 被接受样本数（coverage）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    val_accs_max.append(100 * cm_max / total_max)\n",
    "    val_accs_pred.append(100 * cm_pred / total_pred if total_pred > 0 else 0.0)\n",
    "    cov = 100 * (total_pred / total_max) if total_max > 0 else 0.0  # Coverage（被接收比例）\n",
    "    coverages.append(cov)\n",
    "\n",
    "    # —— dynamic weight update ——\n",
    "    if epoch >= 1:\n",
    "        all_p, all_t = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                all_p.append(out.argmax(dim=1))\n",
    "                all_t.append(yb)\n",
    "        wp = tracker.update(torch.cat(all_p), torch.cat(all_t))\n",
    "        criterion = nn.CrossEntropyLoss(weight=wp.to(device))\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | \"\n",
    "          f\"Train Acc: {train_accs[-1]:.2f}% | \"\n",
    "          f\"Val Acc(max): {val_accs_max[-1]:.2f}% | \"\n",
    "          f\"Val Acc(pred): {val_accs_pred[-1]:.2f}% | \"\n",
    "          f\"Coverage: {coverages[-1]:.1f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------- 曲线可视化 ---------------------------\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_accs, label=\"Train Acc\")\n",
    "plt.plot(val_accs_max, label=\"Val Acc (max)\")\n",
    "plt.plot(val_accs_pred, label=\"Val Acc (predict, with rejection)\")\n",
    "plt.plot(coverages, label=\"Coverage (%)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy / %\")\n",
    "plt.title(\"Training Curve: TCN (pool every 2) + CLS-Attn + ArcFace\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "###\n",
    "\n",
    "\n",
    "# ================= Validation: threshold calibration + per-class gate stats + report =================\n",
    "import os, json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# ----------------- A) 配置：选择校准策略 -----------------\n",
    "CAL_POLICY      = \"target_coverage\"   # 可选: \"target_coverage\" 或 \"target_risk\"\n",
    "TARGET_VALUE    = 0.65                # policy=\"target_coverage\" -> 覆盖率目标(0~1)；policy=\"target_risk\" -> 风险=1-准确率\n",
    "CALIB_BATCHSIZE = 512                 # 校准批大小\n",
    "PLOT_CALIB      = True                # 是否绘制 置信度直方图 + 风险-覆盖率曲线\n",
    "SAVE_JSON_PATH  = \"./val_gate_stats.json\"  # 门控参数缓存文件（供代码1直接读取）\n",
    "\n",
    "# ----------------- B) 在验证集上标定阈值（风险-覆盖率） -----------------\n",
    "@torch.no_grad()\n",
    "def calibrate_threshold_on_val(model, X_val, y_val,\n",
    "                               device=\"cpu\", batch_size=512,\n",
    "                               policy=\"target_coverage\", target=0.90,\n",
    "                               plot=True):\n",
    "    \"\"\"\n",
    "    选择性分类标定 Softmax 阈值：\n",
    "      - 对每个样本取最大置信度 conf 与是否预测正确 correct。\n",
    "      - 按 conf 降序累加 -> (coverage, acc_on_accepted, risk) 曲线。\n",
    "      - policy=\"target_coverage\": 选 coverage >= target 时风险最小的点；\n",
    "        policy=\"target_risk\": 选 risk <= target 时覆盖率最大的点。\n",
    "    返回：thr_star, coverage_star, acc_on_accepted_star，并写回 model.threshold。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    confs, corrects = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        probs  = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(dim=1)\n",
    "        confs.append(conf.cpu())\n",
    "        corrects.append((pred == yb).cpu())\n",
    "\n",
    "    confs   = torch.cat(confs).numpy()                         # [N]\n",
    "    correct = torch.cat(corrects).numpy().astype(np.int32)\n",
    "    N = len(confs)\n",
    "    if N == 0:\n",
    "        raise RuntimeError(\"Validation set is empty, cannot calibrate threshold.\")\n",
    "\n",
    "    # 置信度降序 -> 覆盖率曲线\n",
    "    order = np.argsort(-confs)\n",
    "    conf_sorted    = confs[order]\n",
    "    correct_sorted = correct[order]\n",
    "\n",
    "    k = np.arange(1, N+1)\n",
    "    coverage = k / N\n",
    "    acc_on_accepted = np.cumsum(correct_sorted) / k\n",
    "    risk = 1.0 - acc_on_accepted\n",
    "\n",
    "    # 选点\n",
    "    if policy == \"target_coverage\":\n",
    "        idx0 = int(np.searchsorted(coverage, target, side=\"left\"))\n",
    "        idx0 = np.clip(idx0, 0, N-1)\n",
    "        tail = np.arange(idx0, N)\n",
    "        idx_star = int(tail[np.argmin(risk[tail])])  # 该覆盖率及右侧中风险最小\n",
    "    elif policy == \"target_risk\":\n",
    "        ok = np.where(risk <= target)[0]\n",
    "        idx_star = int(ok[-1]) if ok.size > 0 else int(np.argmin(risk))\n",
    "    else:\n",
    "        raise ValueError(\"CAL_POLICY must be 'target_coverage' or 'target_risk'.\")\n",
    "\n",
    "    thr_star = float(conf_sorted[idx_star])\n",
    "    cov_star = float(coverage[idx_star])\n",
    "    acc_star = float(acc_on_accepted[idx_star])\n",
    "\n",
    "    # 写回阈值\n",
    "    model.threshold = thr_star\n",
    "\n",
    "    print(\"\\n[Calibration]\")\n",
    "    print(f\"  policy = {policy}, target = {target}\")\n",
    "    print(f\"  -> threshold = {thr_star:.4f}\")\n",
    "    print(f\"  -> coverage  = {cov_star*100:.2f}%\")\n",
    "    print(f\"  -> acc_on_accepted (1-risk) = {acc_star*100:.2f}%\")\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(12,4.5))\n",
    "        ax1 = fig.add_subplot(1,2,1)\n",
    "        ax1.hist(confs[correct==1], bins=40, alpha=0.6, label=\"Correct\", density=True)\n",
    "        ax1.hist(confs[correct==0], bins=40, alpha=0.6, label=\"Incorrect\", density=True)\n",
    "        ax1.axvline(thr_star, ls=\"--\", c=\"k\", label=f\"Threshold {thr_star:.2f}\")\n",
    "        ax1.set_xlabel(\"Max softmax confidence\")\n",
    "        ax1.set_ylabel(\"Density\")\n",
    "        ax1.set_title(\"Validation confidence distribution\")\n",
    "        ax1.legend()\n",
    "\n",
    "        # —— 把风险-覆盖率改为“接收样本上的准确率（Precision/Accuracy on accepted）-覆盖率” —— #\n",
    "        ax2 = fig.add_subplot(1,2,2)\n",
    "        ax2.plot(coverage, acc_on_accepted, lw=2)\n",
    "        ax2.scatter([cov_star], [acc_star], s=50, edgecolors=\"k\",\n",
    "                    label=f\"Chosen: cov={cov_star*100:.1f}%, acc={acc_star*100:.2f}%\")\n",
    "        ax2.set_xlabel(\"Coverage (accepted proportion)\")\n",
    "        ax2.set_ylabel(\"Accuracy on accepted (Precision)\")\n",
    "        ax2.set_title(\"Accuracy–Coverage curve (validation)\")\n",
    "        ax2.set_xlim(0.2, 1)\n",
    "        ax2.set_ylim(0.92, 1)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return thr_star, cov_star, acc_star\n",
    "\n",
    "# ----------------- C) 先标定阈值 -----------------\n",
    "thr_star, cov_star, acc_star = calibrate_threshold_on_val(\n",
    "    model, X_val, y_val,\n",
    "    device=device,\n",
    "    batch_size=CALIB_BATCHSIZE,\n",
    "    policy=CAL_POLICY,\n",
    "    target=TARGET_VALUE,\n",
    "    plot=PLOT_CALIB\n",
    ")\n",
    "print(f\"[Model] model.threshold 已设置为 {model.threshold:.4f}\")\n",
    "\n",
    "# ----------------- D) 用选定阈值统计每类 gate 参数 (r_c, a_c, S_1mr, S_full) 并缓存 -----------------\n",
    "@torch.no_grad()\n",
    "def compute_val_gate_stats(model, X_val, y_val, thr, device=\"cpu\", batch_size=512):\n",
    "    model.eval()\n",
    "    loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "    y_true_list, y_hat_list, p_max_list = [], [], []\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        probs  = F.softmax(logits, dim=1)\n",
    "        p_max, y_hat = probs.max(dim=1)\n",
    "        y_true_list.append(yb.cpu()); y_hat_list.append(y_hat.cpu()); p_max_list.append(p_max.cpu())\n",
    "    y_true = torch.cat(y_true_list).numpy()\n",
    "    y_hat  = torch.cat(y_hat_list).numpy()\n",
    "    p_max  = torch.cat(p_max_list).numpy()\n",
    "    num_classes = int(max(y_true.max(), y_hat.max())) + 1\n",
    "\n",
    "    accepted = (p_max >= thr)\n",
    "    gate = {\"selected_threshold\": float(thr), \"per_class\": {}, \"overall\": {}}\n",
    "\n",
    "    # overall\n",
    "    acc_on_accepted = float((y_hat[accepted] == y_true[accepted]).mean()) if accepted.any() else 0.0\n",
    "    coverage = float(accepted.mean())\n",
    "    gate[\"overall\"] = {\"acc_on_accepted\": acc_on_accepted, \"coverage\": coverage}\n",
    "\n",
    "    # per-class\n",
    "    for c in range(num_classes):\n",
    "        is_c = (y_true == c)\n",
    "        n_c  = int(is_c.sum())\n",
    "        if n_c == 0:\n",
    "            gate[\"per_class\"][str(c)] = {\"r\": None, \"a\": None, \"S_1mr\": None, \"S_full\": None}\n",
    "            continue\n",
    "        # r_c: 真 c 被拒识比例\n",
    "        rc = float((~accepted[is_c]).mean())\n",
    "        # a_c: 被接收的真 c 中预测为 c 的准确率\n",
    "        mask_acc_c = accepted & is_c\n",
    "        if mask_acc_c.any():\n",
    "            ac = float((y_hat[mask_acc_c] == c).mean())\n",
    "        else:\n",
    "            ac = 0.0\n",
    "        gate[\"per_class\"][str(c)] = {\n",
    "            \"r\": rc,\n",
    "            \"a\": ac,\n",
    "            \"S_1mr\": 1.0 - rc,\n",
    "            \"S_full\": (1.0 - rc) * ac\n",
    "        }\n",
    "    return gate\n",
    "\n",
    "VAL_GATE_STATS = compute_val_gate_stats(model, X_val, y_val, thr_star, device=device, batch_size=CALIB_BATCHSIZE)\n",
    "VAL_SELECTED_THR = float(thr_star)\n",
    "\n",
    "# 保存到 json，供 Monte-Carlo/GCY 模块直接复用\n",
    "with open(SAVE_JSON_PATH, \"w\") as f:\n",
    "    json.dump(VAL_GATE_STATS, f, indent=2)\n",
    "print(f\"[Saved] per-class gate stats -> {os.path.abspath(SAVE_JSON_PATH)}\")\n",
    "\n",
    "# ----------------- E) 用选定阈值输出逐类指标 & （可选）混淆矩阵 -----------------\n",
    "# 重新前向（也可改为复用上面结果）\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=256, shuffle=False)\n",
    "y_true_list, y_hat_list, p_max_list = [], [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        probs  = F.softmax(logits, dim=1)\n",
    "        p_max, y_hat = probs.max(dim=1)\n",
    "        y_true_list.append(yb.cpu()); y_hat_list.append(y_hat.cpu()); p_max_list.append(p_max.cpu())\n",
    "y_true = torch.cat(y_true_list).numpy()\n",
    "y_hat  = torch.cat(y_hat_list).numpy()\n",
    "p_max  = torch.cat(p_max_list).numpy()\n",
    "\n",
    "accepted_mask = (p_max >= VAL_SELECTED_THR)\n",
    "num_classes = int(max(y_true.max(), y_hat.max())) + 1\n",
    "\n",
    "print(\"\\n==== Accuracy and Uncertainty Per Label (Validation @ selected threshold) ====\")\n",
    "overall_correct_conf = 0\n",
    "overall_total_conf   = 0\n",
    "overall_not_pred     = 0\n",
    "\n",
    "for c in range(num_classes):\n",
    "    mask_c  = (y_true == c)\n",
    "    total_c = int(mask_c.sum())\n",
    "    if total_c == 0:\n",
    "        continue\n",
    "\n",
    "    acc_max = float((y_hat[mask_c] == c).mean()) * 100.0\n",
    "\n",
    "    mask_acc = mask_c & accepted_mask\n",
    "    accept_c = int(mask_acc.sum())\n",
    "    if accept_c > 0:\n",
    "        acc_conf = float((y_hat[mask_acc] == c).mean()) * 100.0\n",
    "        correct_c = int((y_hat[mask_acc] == c).sum())\n",
    "        acc_conf_display = f\"{acc_conf:5.2f}%\"\n",
    "    else:\n",
    "        acc_conf_display = \"  NaN\"\n",
    "        correct_c = 0\n",
    "\n",
    "    not_pred_pct = float((~accepted_mask[mask_c]).mean()) * 100.0\n",
    "\n",
    "    overall_correct_conf += correct_c\n",
    "    overall_total_conf   += accept_c\n",
    "    overall_not_pred     += int((~accepted_mask[mask_c]).sum())\n",
    "\n",
    "    print(f\"Label {c:02d}: 总样本={total_c:4d}, \"\n",
    "          f\"argmax Acc={acc_max:5.2f}%, \"\n",
    "          f\"阈值 Acc={acc_conf_display:>6}, \"\n",
    "          f\"Not Predicted={not_pred_pct:5.2f}% \"\n",
    "          f\"(正确 {correct_c}/{accept_c})\")\n",
    "\n",
    "overall_acc_conf = (overall_correct_conf / overall_total_conf * 100.0) if overall_total_conf > 0 else 0.0\n",
    "overall_not_pred_pct = overall_not_pred / y_true.size * 100.0\n",
    "print(f\"\\nOverall Accuracy (on accepted): {overall_acc_conf:.2f}%\")\n",
    "print(f\"Overall Not Predicted Percentage: {overall_not_pred_pct:.2f}%\")\n",
    "print(f\"Selected threshold: {VAL_SELECTED_THR:.4f}\")\n",
    "\n",
    "# ——（可选）只在“被接收”的样本上画混淆矩阵 —— #\n",
    "PLOT_CM = True\n",
    "if PLOT_CM:\n",
    "    valid = accepted_mask\n",
    "    y_true_f = y_true[valid]\n",
    "    y_pred_f = y_hat[valid]\n",
    "    if y_true_f.size > 0:\n",
    "        labels_order = sorted(list(set(y_true_f.tolist()) | set(y_pred_f.tolist())))\n",
    "        cm = confusion_matrix(y_true_f, y_pred_f, labels=labels_order)\n",
    "\n",
    "        plt.figure(figsize=(12, 9))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=labels_order, yticklabels=labels_order)\n",
    "        plt.xlabel('Predicted Label'); plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix on Accepted Samples (Validation)')\n",
    "        plt.show()\n",
    "\n",
    "        cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "        plt.figure(figsize=(12, 9))\n",
    "        sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                    xticklabels=labels_order, yticklabels=labels_order)\n",
    "        plt.xlabel('Predicted Label'); plt.ylabel('True Label')\n",
    "        plt.title('Normalized Confusion Matrix on Accepted Samples (Validation)')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"（所有验证样本都被拒识，跳过混淆矩阵绘制）\")\n",
    "# ====================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ====== 在绘图之后，替换之前的一次性 model(X_val) 评估，改为分批评估 ======\n",
    "model.eval()\n",
    "\n",
    "# 创建验证集 DataLoader，batch_size 根据你的显存调整，32 或者更小\n",
    "val_dataset = TensorDataset(X_test, y_test)\n",
    "val_loader  = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "all_true       = []\n",
    "all_pred_max   = []\n",
    "all_pred_conf  = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)                         # [B, num_classes]\n",
    "        _, pm = torch.max(out, dim=1)           # 普通 argmax\n",
    "        pc, _ = model.predict(xb)               # 带阈值预测\n",
    "\n",
    "        all_true.append(yb.cpu())\n",
    "        all_pred_max.append(pm.cpu())\n",
    "        all_pred_conf.append(pc.cpu())\n",
    "\n",
    "all_true      = torch.cat(all_true)\n",
    "all_pred_max  = torch.cat(all_pred_max)\n",
    "all_pred_conf = torch.cat(all_pred_conf)\n",
    "\n",
    "print(\"\\n=== 测试集上各标签准确度（分批计算） ===\")\n",
    "num_classes = all_pred_max.max().item() + 1\n",
    "for c in range(num_classes):\n",
    "    mask_c = (all_true == c)\n",
    "    total_c = mask_c.sum().item()\n",
    "    if total_c == 0:\n",
    "        continue\n",
    "\n",
    "    # 普通 argmax 准确度\n",
    "    correct_max_c = (all_pred_max[mask_c] == c).sum().item()\n",
    "    acc_max = correct_max_c / total_c * 100\n",
    "\n",
    "    # 带阈值的准确度（只对那些预测 != -1 的样本算）\n",
    "    mask_conf_c = mask_c & (all_pred_conf != -1)\n",
    "    count_conf_c = mask_conf_c.sum().item()\n",
    "    if count_conf_c > 0:\n",
    "        correct_conf_c = (all_pred_conf[mask_conf_c] == c).sum().item()\n",
    "        acc_conf = correct_conf_c / count_conf_c * 100\n",
    "    else:\n",
    "        acc_conf = float('nan')\n",
    "        correct_conf_c = 0\n",
    "\n",
    "    print(f\"Label {c:02d}: \"\n",
    "          f\" 总样本={total_c:3d}, \"\n",
    "          f\"argmax Acc={acc_max:5.2f}%, \"\n",
    "          f\"阈值 Acc={acc_conf if not np.isnan(acc_conf) else ' NaN':6}%, \"\n",
    "          f\"(正确 {correct_conf_c}/{count_conf_c})\")\n",
    "\n",
    "\n",
    "# 评估模型（预测、准确率、未预测比例）\n",
    "def evaluate_model_per_label_before_cm(model, test_signals, test_labels, device='cpu', batch_size=32):\n",
    "    model.eval()\n",
    "    unique_labels = torch.unique(test_labels).tolist()\n",
    "\n",
    "    correct_per_label = {label: 0 for label in unique_labels}\n",
    "    total_per_label = {label: 0 for label in unique_labels}\n",
    "    not_predicted_per_label = {label: 0 for label in unique_labels}\n",
    "\n",
    "    total_correct_all = 0\n",
    "    total_samples_all = 0\n",
    "    total_not_predicted_all = 0\n",
    "\n",
    "    test_signals = test_signals.to(device)\n",
    "    test_labels = test_labels.to(device)\n",
    "\n",
    "    confident_predictions_all = []\n",
    "    confident_labels_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, test_signals.size(0), batch_size):\n",
    "            input_batch = test_signals[i:i + batch_size]\n",
    "            target_batch = test_labels[i:i + batch_size]\n",
    "\n",
    "            outputs = model(input_batch)\n",
    "            probabilities = F.softmax(outputs, dim=1)   # ← 在这里加一行\n",
    "            confident_probs, confident_predictions = torch.max(probabilities, dim=1)\n",
    "            confident_predictions[confident_probs < model.threshold] = -1\n",
    "\n",
    "            confident_predictions_all.append(confident_predictions)\n",
    "            confident_labels_all.append(target_batch)\n",
    "\n",
    "            for label in unique_labels:\n",
    "                mask = target_batch == label\n",
    "                correct_per_label[label] += (confident_predictions[mask] == target_batch[mask]).sum().item()\n",
    "                total_per_label[label] += mask.sum().item()\n",
    "                not_predicted_per_label[label] += (confident_predictions[mask] == -1).sum().item()\n",
    "\n",
    "            total_correct_all += (confident_predictions == target_batch).sum().item()\n",
    "            total_samples_all += target_batch.size(0)\n",
    "            total_not_predicted_all += (confident_predictions == -1).sum().item()\n",
    "\n",
    "    confident_predictions_all = torch.cat(confident_predictions_all)\n",
    "    confident_labels_all = torch.cat(confident_labels_all)\n",
    "\n",
    "    accuracy_per_label = {\n",
    "        label: (correct_per_label[label] / total_per_label[label]) * 100 if total_per_label[label] > 0 else 0\n",
    "        for label in total_per_label\n",
    "    }\n",
    "\n",
    "    not_predicted_per_label_percent = {\n",
    "        label: (not_predicted_per_label[label] / total_per_label[label]) * 100 if total_per_label[label] > 0 else 0\n",
    "        for label in total_per_label\n",
    "    }\n",
    "\n",
    "    overall_accuracy = (total_correct_all / total_samples_all) * 100 if total_samples_all > 0 else 0\n",
    "    overall_not_predicted_percent = (total_not_predicted_all / total_samples_all) * 100 if total_samples_all > 0 else 0\n",
    "\n",
    "    return accuracy_per_label, not_predicted_per_label_percent, overall_accuracy, overall_not_predicted_percent, confident_predictions_all, confident_labels_all\n",
    "\n",
    "\n",
    "# 评估并绘制混淆矩阵\n",
    "def evaluate_model_per_label_after_cm(model, test_signals, test_labels, device='cpu', batch_size=32):\n",
    "    accuracy_per_label, not_predicted_per_label_percent, overall_accuracy, overall_not_predicted_percent, confident_predictions_all, confident_labels_all = evaluate_model_per_label_before_cm(\n",
    "        model, test_signals, test_labels, device=device, batch_size=batch_size)\n",
    "\n",
    "    valid_mask = confident_predictions_all != -1\n",
    "    filtered_predicted = confident_predictions_all[valid_mask]\n",
    "    filtered_true_labels = confident_labels_all[valid_mask]\n",
    "\n",
    "    cm = confusion_matrix(filtered_true_labels.cpu().numpy(), filtered_predicted.cpu().numpy())\n",
    "    unique_labels = sorted(list(set(filtered_true_labels.cpu().numpy()) | set(filtered_predicted.cpu().numpy())))\n",
    "\n",
    "    correct_per_label = {label: 0 for label in unique_labels}\n",
    "    total_per_label = {label: 0 for label in unique_labels}\n",
    "    not_predicted_per_label = {label: 0 for label in unique_labels}\n",
    "\n",
    "    for label in unique_labels:\n",
    "        mask = filtered_true_labels == label\n",
    "        correct_per_label[label] = (filtered_predicted[mask] == filtered_true_labels[mask]).sum().item()\n",
    "        total_per_label[label] = mask.sum().item()\n",
    "        not_predicted_per_label[label] = (filtered_predicted[mask] == -1).sum().item()\n",
    "\n",
    "    total_correct_all = (filtered_predicted == filtered_true_labels).sum().item()\n",
    "    total_samples_all = filtered_true_labels.size(0)\n",
    "    total_not_predicted_all = (filtered_predicted == -1).sum().item()\n",
    "\n",
    "    accuracy_per_label_after_cm = {\n",
    "        label: (correct_per_label[label] / total_per_label[label]) * 100 if total_per_label[label] > 0 else 0\n",
    "        for label in total_per_label\n",
    "    }\n",
    "\n",
    "    not_predicted_per_label_percent_after_cm = {\n",
    "        label: (not_predicted_per_label[label] / total_per_label[label]) * 100 if total_per_label[label] > 0 else 0\n",
    "        for label in total_per_label\n",
    "    }\n",
    "\n",
    "    overall_accuracy_after_cm = (total_correct_all / total_samples_all) * 100 if total_samples_all > 0 else 0\n",
    "    overall_not_predicted_percent_after_cm = (total_not_predicted_all / total_samples_all) * 100 if total_samples_all > 0 else 0\n",
    "\n",
    "    return accuracy_per_label_after_cm, not_predicted_per_label_percent_after_cm, overall_accuracy_after_cm, overall_not_predicted_percent_after_cm, cm, unique_labels\n",
    "\n",
    "\n",
    "# ======= 用法示例（假设你已经有 model、X_test、y_test）=======\n",
    "\n",
    "# Step 1: 初步评估\n",
    "acc1, np1, overall_acc1, overall_np1, preds, labels = evaluate_model_per_label_before_cm(\n",
    "    model, X_test, y_test, device=device)\n",
    "\n",
    "print(\"\\n==== Accuracy and Uncertainty Per Label ====\")\n",
    "for label in acc1:\n",
    "    print(f'Label {label}: Accuracy = {acc1[label]:.2f}%, Not Predicted = {np1[label]:.2f}%')\n",
    "\n",
    "print(f'\\nOverall Accuracy: {overall_acc1:.2f}%')\n",
    "print(f'Overall Not Predicted Percentage: {overall_np1:.2f}%')\n",
    "\n",
    "\n",
    "# Step 2: 混淆矩阵分析\n",
    "acc2, np2, overall_acc2, overall_np2, cm, label_order = evaluate_model_per_label_after_cm(\n",
    "    model, X_test, y_test, device=device)\n",
    "\n",
    "# ====== 在这里打印具体数值 ======\n",
    "print(\"\\n==== Test Confusion Matrix (absolute counts) ====\")\n",
    "print(\"Label order (rows = true, cols = pred):\", label_order)\n",
    "print(cm)\n",
    "\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "print(\"\\n==== Test Confusion Matrix (row-normalized) ====\")\n",
    "print(cm_normalized)\n",
    "# ====== 打印结束，下面保持原来的画图 ======\n",
    "\n",
    "# 绘图：混淆矩阵\n",
    "plt.figure(figsize=(12, 9))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=label_order, yticklabels=label_order)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix (Absolute Values)')\n",
    "plt.show()\n",
    "\n",
    "# 绘图：归一化混淆矩阵\n",
    "plt.figure(figsize=(12, 9))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "            xticklabels=label_order, yticklabels=label_order)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix (Normalized)')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
